{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Logistic Regression as Neuron\n",
    "Logistic Regression is the algorithm used for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LR.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First we shall create a toy dataset\n",
    "This toy dataset will be easy to visualize what is happening\n",
    "\n",
    "Let say we want to classify data as either it is <strong> A Mango </strong> or <strong>  Not A Mango </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "            # Weight, Class 1 = Mango and Class 0 = Not-Mango\n",
    "data = np.array([[50, 1], \n",
    "                 [75, 1],\n",
    "                 [100, 0],\n",
    "                 [150, 0]])\n",
    "\n",
    "# clearly we can see that if the weight < 100 it is a mango and weight >= 100 not a mango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (1, 4)\n",
      "Shape of y: (1, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the examples with features and their class labels from data\n",
    "# x = examples and y = class label for each example\n",
    "x = data[:, 0].reshape(1, 4)         # reshape is used to avoid rank 1 array\n",
    "y = data[:, 1].reshape(1, 4) \n",
    "print(\"Shape of x:\", x.shape)        # feel free to try without reshape and check the shape\n",
    "print(\"Shape of y:\",y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the parameters\n",
    "Note: We can initialize the weights and bias in the LR to Zero but for NN we should do Random Initialization for weights, else the weights corresponding to inputs going into each hidden unit areidentical and prevents NN to learn any new feature\n",
    "- Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  [[0.]]\n",
      "Shape of W:  (1, 1)\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros((x.shape[0], 1)) # feel free to randomly initialize the w\n",
    "b = 0 \n",
    "\n",
    "print(\"w: \", w)\n",
    "print(\"Shape of W: \", w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "We shall use \"Sigmoid\" activation function \n",
    "\n",
    "$$ sigmoid = \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]]\n",
      "Computed Sigmoid:  [[0.5 0.5 0.5 0.5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "# linear computation\n",
    "z = np.dot(w.T, x) + b\n",
    "print(z)\n",
    "\n",
    "# using sigmoid\n",
    "A = sigmoid(z)\n",
    "print(\"Computed Sigmoid: \",A)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Cost Function\n",
    "$$ J(\\theta) =  \\frac{-1}{m} \\left[ \\sum_ {i = 1}^{m} y^{(i)}\\log(h_\\theta(x^{(i)}) + (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)})) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 0\n",
    "# a = 0.5\n",
    "# loss = y * np.log(a) + (1-y)*np.log(1-a)\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# Calculating the cost\n",
    "m = x.shape[1] # total number of examples in the data is given by x.shape[1] = 4\n",
    "costs = [] # list to maintain cost for each iteration otherwise it would replace the value in cost variable\n",
    "cost = - np.sum(( y*np.log(A) + ((1-y)*np.log(1-A)))) / m\n",
    "costs.append(cost)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "<img src=\"Backprop.gif\"/>\n",
    "<strong>\n",
    "1. Loss function $L(\\theta)$ computes error for single training example <br>\n",
    "2. Cost function $J(\\Theta)$ is the average of the loss functions of the entire training set. \n",
    "</strong>\n",
    "\n",
    "- $$ \\frac {\\partial} {\\partial a}L = -\\frac{y}{a} + \\frac{1-y}{1-a}$$\n",
    "- $$ \\frac {\\partial} {\\partial z}L = a - y $$\n",
    "- $$ \\frac {\\partial} {\\partial w}L = x.\\partial z $$\n",
    "- $$ \\frac {\\partial} {\\partial b}L = \\partial z $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  [[0.5 0.5 0.5 0.5]]\n",
      "y:  [[1 1 0 0]]\n",
      "dZ:  [[-0.5 -0.5  0.5  0.5]]\n",
      "dW:  [[15.625]]\n",
      "db:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Backward propagation\n",
    "# dL/da = -(y/a) + ((1-y)/(1-a))\n",
    "\n",
    "print(\"A: \",A)\n",
    "print(\"y: \",y)\n",
    "\n",
    "dz = (A - y)                  # dL/dz = a - y\n",
    "dw = np.dot(x, dz.T) / m      # dL/dw = x.dz\n",
    "db = np.sum(dz) / m           # dL/db = dz\n",
    "print(\"dZ: \",dz)\n",
    "print(\"dW: \",dw)\n",
    "print(\"db: \",db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checking\n",
    "$$\\frac {\\partial}{\\partial \\theta} J(\\theta) \\approx \\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon} , \\epsilon = 10^{-4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "epsilon = 10**(-4)\n",
    "print(w.shape)\n",
    "\n",
    "def compute_cost(weights, bias, x, y, m):\n",
    "    z = np.dot(weights.T, x) + bias\n",
    "    A = sigmoid(z)\n",
    "    cost = - np.sum(( y*np.log(A) + ((1-y)*np.log(1-A)) )) / m\n",
    "    return cost\n",
    "    \n",
    "def grad_check(weights, bias, x, y, m):\n",
    "    pos_weights = np.copy(weights)\n",
    "    neg_weights = np.copy(weights)\n",
    "    \n",
    "    pos_weights[0] = pos_weights[0] + epsilon\n",
    "    neg_weights[0] = neg_weights[0] - epsilon\n",
    "    \n",
    "    compute_pos_cost = compute_cost(pos_weights, bias, x, y, m)\n",
    "#     print(compute_pos_cost)\n",
    "    compute_neg_cost = compute_cost(neg_weights, bias, x, y, m)\n",
    "#     print(compute_neg_cost)\n",
    "    grad_cost = (compute_pos_cost - compute_neg_cost) / (2 * epsilon)\n",
    "    return grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.624999999999666"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the value of dW from above cell\n",
    "grad_check(w, b, x, y, m)\n",
    "\n",
    "# we can see that the values are approx. same, hence the implementation of backpropagation is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize / Update parameters\n",
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the parameters \n",
    "learning_rate = 0.001\n",
    "w = w - learning_rate * dw\n",
    "b = b - learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.015625]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# updated parameters\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31405054, 0.23651624, 0.17328821, 0.08756384]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting the training set with new parameters value\n",
    "predict = sigmoid(np.dot(w.T, x) + b)\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that training was poor as we wanted it to output [1, 1, 0, 0]\n",
    "\n",
    "## 2nd iteration \n",
    "we shall use the previous weights and bias that was calculated to perform this iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-15.27383962]]\n",
      "-15.273687042650641\n",
      "Predict:  [[0.49568489 0.4934904  0.49129616 0.48690876]]\n",
      "Costs:  [0.6931471805599453, 0.7204690131374397]\n"
     ]
    }
   ],
   "source": [
    "# carry forward the w and b \n",
    "z = np.dot(w.T, x) + b\n",
    "A = sigmoid(z)\n",
    "\n",
    "# compute the cost\n",
    "cost = - np.sum(( y*np.log(A) + ((1-y)*np.log(1-A)))) / m\n",
    "costs.append(cost)\n",
    "\n",
    "# backpropagation\n",
    "dz = (A - y)                  \n",
    "dw = np.dot(x, dz.T) / m  \n",
    "# uncomment below two lines to perform gradient checking \n",
    "# print(dw)\n",
    "# print(grad_check(w, b, x, y, m))\n",
    "db = np.sum(dz) / m           \n",
    "\n",
    "# update the parameteres\n",
    "w = w - learning_rate * dw\n",
    "b = b - learning_rate * db\n",
    "\n",
    "# predict the training set\n",
    "predict = sigmoid(np.dot(w.T, x) + b)\n",
    "print(\"Predict: \", predict)\n",
    "print(\"Costs: \", costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict:  [[0.31989005 0.24387178 0.18110786 0.09419627]]\n",
      "Costs:  [0.6931471805599453, 0.7204690131374397, 0.6878144031746712]\n"
     ]
    }
   ],
   "source": [
    "# carry forward the w and b \n",
    "z = np.dot(w.T, x) + b\n",
    "A = sigmoid(z)\n",
    "\n",
    "# compute the cost\n",
    "cost = - np.sum(( y*np.log(A) + ((1-y)*np.log(1-A)))) / m\n",
    "costs.append(cost)\n",
    "\n",
    "# backpropagation\n",
    "dz = (A - y)                  \n",
    "dw = np.dot(x, dz.T) / m      \n",
    "db = np.sum(dz) / m           \n",
    "\n",
    "# update the parameteres\n",
    "w = w - learning_rate * dw\n",
    "b = b - learning_rate * db\n",
    "\n",
    "# predict the training set\n",
    "predict = sigmoid(np.dot(w.T, x) + b)\n",
    "print(\"Predict: \", predict)\n",
    "print(\"Costs: \", costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7124065426563126\n",
      "5000 0.5606916012290944\n",
      "10000 0.48369676691299734\n",
      "15000 0.4329869696643792\n",
      "20000 0.39380795672215\n",
      "25000 0.36280721422234197\n",
      "30000 0.33771619182749435\n",
      "35000 0.31698682328018823\n",
      "40000 0.2995459576565257\n",
      "45000 0.28463556038115834\n",
      "Final prediction:  [[0.84076563 0.60911763 0.31502586 0.03851658]]\n"
     ]
    }
   ],
   "source": [
    "# doing the same operation for 50000 iterations\n",
    "for i in range(50000):\n",
    "    # carry forward the w and b \n",
    "    z = np.dot(w.T, x) + b\n",
    "    A = sigmoid(z)\n",
    "\n",
    "    # compute the cost\n",
    "    cost = - np.sum(( y*np.log(A) + ((1-y)*np.log(1-A)))) / m\n",
    "    costs.append(cost)\n",
    "    # prints cost at every 5000th iteration\n",
    "    if i%5000 == 0:\n",
    "        print(i, cost)\n",
    "\n",
    "    # backpropagation\n",
    "    dz = (A - y)                  \n",
    "    dw = np.dot(x, dz.T) / m      \n",
    "    db = np.sum(dz) / m           \n",
    "\n",
    "    # update the parameteres\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "\n",
    "    # predict the training set\n",
    "    predict = sigmoid(np.dot(w.T, x) + b)\n",
    "\n",
    "# checking the final prediction\n",
    "print(\"Final prediction: \", predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
